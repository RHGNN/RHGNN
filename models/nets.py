import torchimport torch.nn.functional as Ffrom torch.nn import Linear, Sequentialfrom models.conv import GCNConv, GATConv, CompGCNConv, CompGCNConvBasis, RGCNConv, RHopConv, \ \GatedConv_n, GatedConv_e, RSHNConv, NodeConv_hop, EdgeConv_hop, EdgeConv_Maskfrom utils.helper import *from torch.nn import Sequential as Seq, Linear as Lin, ReLUfrom torch_scatter import scatter_meanfrom torch.nn import Parameter as Paramfrom utils.process_graph import degree_edge, sort_edge_index, doubly_stochastic_normlization, to_undirected_edge_attr, \ \add_self_loops_with_edge_attrfrom torch_geometric.utils import remove_self_loopsdef str2act(str):    if str == 'elu':        act = torch.nn.ELU()    elif str == 'relu':        act = torch.nn.ReLU()    elif str == 'tanh':        act = torch.nn.Tanh()    elif str == 'identity':        act = torch.nn.Identity()    return actclass GCN(torch.nn.Module):    def __init__(self, args):        super(GCN, self).__init__()        self.in_dim = args.n_nodes        hidden = args.n_dim        num_classes = args.n_classes        self.drop = torch.nn.Dropout(args.dropout)        self.conv1 = GCNConv(self.in_dim, hidden)        self.conv2 = GCNConv(hidden, num_classes)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()    def forward(self, data):        x = F.elu(self.conv1(x=None, edge_index=data.edge_index))        x = self.drop(x)        x = self.conv2(x, data.edge_index)        return F.log_softmax(x, dim=1)    def __repr__(self):        return self.__class__.__name__class GAT(torch.nn.Module):    def __init__(self, args):        super(GAT, self).__init__()        self.in_dim = args.n_nodes        hidden = args.n_dim        heads = args.heads        num_classes = args.n_classes        self.drop = torch.nn.Dropout(args.dropout)        self.conv1 = GATConv(self.in_dim, hidden, heads=heads, dropout=args.dropout)        self.conv2 = GATConv(hidden, num_classes, heads=1, dropout=args.dropout)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()    def forward(self, data):        x = F.elu(self.conv1(x=None, edge_index=data.edge_index))        x = self.drop(x)        x = self.conv2(x, data.edge_index)        return F.log_softmax(x, dim=1)    def __repr__(self):        return self.__class__.__name__class RGCN(torch.nn.Module):    def __init__(self, args):        super(RGCN, self).__init__()        hidden = args.n_dim        num_classes = args.n_classes        num_relations = args.n_relations        num_bases = args.num_bases        self.drop = torch.nn.Dropout(args.dropout)        self.conv1 = RGCNConv(            args.n_nodes, hidden, num_relations, num_bases=num_bases)        self.conv2 = RGCNConv(            hidden, num_classes, num_relations, num_bases=num_bases)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()    def forward(self, data):        edge_index = data.edge_index        edge_type = data.edge_type        x = F.relu(self.conv1(None, edge_index, edge_type))        x = self.drop(x)        x = self.conv2(x, edge_index, edge_type)        return F.log_softmax(x, dim=1)    def __repr__(self):        return self.__class__.__name__class SHop(torch.nn.Module):    def __init__(self, args):        super(SHop, self).__init__()        self.p = args        n_in_feat = args.n_in_dim        dim = args.n_dim        edge_dim = dim        num_classes = args.n_classes        e_in_dim = args.e_in_dim        t_in_dim = args.t_in_dim        num_relations = args.n_relations        num_bases = args.num_bases        self.drop = torch.nn.Dropout(args.dropout)        self.fc1 = torch.nn.Parameter(torch.FloatTensor(n_in_feat, dim))        '''Node_Net'''        nn = Sequential(Linear(edge_dim, dim))        self.NConv1 = RSHNConv(dim, dim, nn, root_weight=False)        if args.n_layer == 2:            self.NConv2 = RSHNConv(dim, dim, nn, root_weight=False)        '''Edge_Net'''        nn2 = Sequential(Linear(2 * dim + edge_dim, edge_dim))        self.EConv1 = EdgeConv_Mask(edge_dim, edge_dim, nn2)        if args.e_layer == 2:            self.EConv2 = EdgeConv_Mask(edge_dim, edge_dim, nn2)        self.fc_node = Linear(dim, num_classes)        self.fc_type = Linear(t_in_dim, dim)        self.reset_parameters()    def reset_parameters(self):        glorot(self.fc1)        self.fc_node.reset_parameters()        self.fc_type.reset_parameters()        self.NConv1.reset_parameters()        self.EConv1.reset_parameters()        if self.p.n_layer == 2: self.NConv2.reset_parameters()        if self.p.e_layer == 2: self.EConv2.reset_parameters()    def forward(self, data):        edge_index, edge_type = data.edge_index, data.edge_type        x = self.drop(torch.spmm(data.x, self.fc1))        type_attr = self.drop(self.fc_type(data.type_attr))        edge_attr = F.embedding(edge_type, type_attr)        edge_attr = torch.tanh(self.EConv1(x=x, edge_index=edge_index, edge_attr=edge_attr, edge_type=edge_type))        edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        x = torch.tanh(self.NConv1(x, edge_index, edge_attr))        x = F.dropout(x, p=self.p.dropout, training=False)        if self.p.e_layer == 2:            edge_attr = torch.tanh(self.EConv2(x=x, edge_index=edge_index, edge_attr=edge_attr, edge_type=edge_type))            edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        if self.p.n_layer == 2:            x = torch.tanh(self.NConv2(x, edge_index, edge_attr))            x = F.dropout(x, p=self.p.dropout, training=False)        x = self.fc_node(x)        return F.log_softmax(x, dim=1), edge_attrclass CompGCN(torch.nn.Module):    def __init__(self, args):        super(CompGCN, self).__init__()        self.p = args        self.act = torch.tanh        n_in_dim = args.n_in_dim        n_dim = args.n_dim        num_rel = args.n_relations        self.init_embed = Param(torch.Tensor(args.n_nodes, n_in_dim))        if self.p.num_bases > 0:            self.init_rel = Param(torch.Tensor(self.p.num_bases, n_dim))            self.conv1 = CompGCNConvBasis(n_in_dim, n_dim, num_rel, self.p.num_bases, act=self.act,                                          params=self.p)        else:            self.init_rel = Param(torch.Tensor(num_rel, n_dim))            self.conv1 = CompGCNConv(n_in_dim, n_dim, num_rel, act=self.act, params=self.p)        self.conv2 = CompGCNConv(n_dim, n_dim, num_rel, act=self.act, params=self.p) if self.p.n_layer == 2 else None        self.fc = Linear(n_dim, args.n_classes)        self.drop = torch.nn.Dropout(self.p.dropout)        self.reset_parameters()    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.fc.reset_parameters()        torch.nn.init.xavier_normal_(self.init_embed)        torch.nn.init.xavier_normal_(self.init_rel)    def forward(self, data):        x = self.init_embed        r = self.init_rel        x, r = self.conv1(x, data.edge_index, data.edge_type, rel_embed=r)        x = self.drop(x)        x, r = self.conv2(x, data.edge_index, data.edge_type, rel_embed=r) if self.p.n_layer == 2 else (x, r)        x = self.drop(x) if self.p.n_layer == 2 else x        out = self.fc(x)        return F.log_softmax(out, dim=1)class MetaLayer(torch.nn.Module):    def __init__(self, args):        super(MetaLayer, self).__init__()        self.p = args        n_in_dim = args.n_in_dim        e_in_dim = args.e_in_dim        n_dim = args.n_dim        e_dim = args.e_dim        self.fc1 = Lin(n_in_dim, n_dim)        self.fc2 = Lin(e_in_dim, e_dim)        self.edge_mlp = Seq(Lin(2 * n_dim + e_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        self.node_mlp_1 = Seq(Lin(n_dim + e_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        self.node_mlp_2 = Seq(Lin(2 * n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        if self.p.e_layer == 2:            self.edge_mlp2 = Seq(Lin(2 * n_dim + e_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        if self.p.n_layer == 2:            self.node_mlp2_1 = Seq(Lin(n_dim + e_dim, n_dim), ReLU(), Lin(n_dim, n_dim))            self.node_mlp2_2 = Seq(Lin(2 * n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        if self.p.n_layer == 3:            self.node_mlp3_1 = Seq(Lin(n_dim + e_dim, n_dim), ReLU(), Lin(n_dim, n_dim))            self.node_mlp3_2 = Seq(Lin(2 * n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        if self.p.e_layer == 3:            self.edge_mlp3 = Seq(Lin(2 * n_dim + e_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        self.fc_node = Lin(n_dim, self.p.n_classes)        self.reset_parameters()    def reset_parameters(self):        self.fc1.reset_parameters()        self.fc2.reset_parameters()        self.fc_node.reset_parameters()        for item in [self.edge_mlp, self.node_mlp_1, self.node_mlp_2]:            if hasattr(item, 'reset_parameters'):                item.reset_parameters()        if self.p.n_layer == 2:            for item in [self.node_mlp2_1, self.node_mlp2_2]:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()        if self.p.e_layer == 2:            for item in self.edge_mlp2:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()        if self.p.n_layer == 3:            for item in [self.node_mlp3_1, self.node_mlp3_2]:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()        if self.p.e_layer == 3:            for item in self.edge_mlp3:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()    def forward(self, data):        row, col = data.edge_index        # n-layer3: 45.56 ± 4.65, n-layer2: 52.78 ± 12.88        x = F.dropout(self.fc1(data.x), p=self.p.drop_n, training=False)        edge_attr = F.dropout(self.fc2(data.edge_attr), p=self.p.drop_e, training=False)        "edge layer"        src = x[row]        dest = x[col]        edge_attr = torch.cat([src, dest, edge_attr], dim=1)        edge_attr = self.edge_mlp(edge_attr)        edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        "node layer"        out = torch.cat([x[row], edge_attr], dim=1)        out = self.node_mlp_1(out)        out = scatter_mean(out, col, dim=0, dim_size=x.size(0))        out = torch.cat([x, out], dim=1)        x = self.node_mlp_2(out)        x = F.dropout(x, p=self.p.drop_n)        "edge layer 2"        if self.p.e_layer == 2:            src = x[row]            dest = x[col]            edge_attr = torch.cat([src, dest, edge_attr], dim=1)            edge_attr = self.edge_mlp2(edge_attr)            edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        "node layer 2"        if self.p.n_layer == 2:            out = torch.cat([x[row], edge_attr], dim=1)            out = self.node_mlp2_1(out)            out = scatter_mean(out, col, dim=0, dim_size=x.size(0))            out = torch.cat([x, out], dim=1)            x = self.node_mlp2_2(out)            x = F.dropout(x, p=self.p.drop_n)        "edge layer 3"        if self.p.e_layer == 3:            src = x[row]            dest = x[col]            edge_attr = torch.cat([src, dest, edge_attr], dim=1)            edge_attr = self.edge_mlp3(edge_attr)            edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        "node layer 3"        if self.p.n_layer == 3:            x_msg = torch.cat([x[row], edge_attr], dim=1)            x_msg = self.node_mlp3_1(x_msg)            x_msg = scatter_mean(x_msg, col, dim=0, dim_size=x.size(0))            x = torch.cat([x, x_msg], dim=1)            x = self.node_mlp3_2(x)            x = F.dropout(x, p=self.p.drop_n)        x = self.fc_node(x)        return F.log_softmax(x, dim=1)class GatedGCN(torch.nn.Module):    def __init__(self, args):        super(GatedGCN, self).__init__()        self.p = args        self.act = torch.tanh        self.drop_e = torch.nn.Dropout(args.drop_e)        self.drop_n = torch.nn.Dropout(args.drop_n)        n_in_dim = args.n_nodes        e_in_dim = args.e_in_dim        dim = args.n_dim        num_classes = args.n_classes        self.fc_edge = Linear(e_in_dim, dim)        self.x0 = Param(torch.Tensor(n_in_dim, dim))        self.nconv1 = GatedConv_n(dim, dim, dim, args)        self.econv1 = GatedConv_e(dim, dim, dim, args)        self.nconv2 = GatedConv_n(dim, dim, dim, args)        self.econv2 = GatedConv_e(dim, dim, dim, args)        self.fc_node = Linear(dim, num_classes)    def reset_parameters(self):        glorot(self.x0)        self.nconv1.reset_parameters()        self.nconv2.reset_parameters()        self.econv1.reset_parameters()        self.econv2.reset_parameters()        self.fc_node.reset_parameters()        self.fc_edge.reset_parameters()    def norm_edge(self, edge_attr, edge_index):        row, col = edge_index        eps = 1e-5        e = torch.sigmoid(edge_attr)        tmp = degree_edge(e, col)[col]  # Norm by in-degree.        in_norm = 1. / (tmp + eps)        e = e * in_norm        return e    def forward(self, data):        edge_index, edge_type = data.edge_index, data.edge_type        e0 = self.drop_e(self.fc_edge(data.edge_attr))        x0 = self.drop_n(self.x0)        '''conv1'''        e1 = self.econv1(x0, e0, edge_index, edge_type) + e0        e1 = self.drop_e(e1)        # e1 = self.norm_edge(e1, edge_index)        x1 = self.nconv1(x0, e0, edge_index, edge_type) + x0        x1 = self.drop_n(x1)        '''conv2'''        e2 = self.econv2(x1, e1, edge_index, edge_type) + e1        e2 = self.drop_e(e2)        # e2 = self.norm_edge(e2, edge_index)        x2 = self.nconv2(x1, e2, edge_index, edge_type) + x1        x2 = self.drop_n(x2)        out = self.fc_node(x2)        # return F.log_softmax(out, dim=1), e2, x2, None        return F.log_softmax(out, dim=1)    def __repr__(self):        return self.__class__.__name__class RHop(torch.nn.Module):    def __init__(self, args):        super(RHop, self).__init__()        self.p = args        self.act = torch.tanh        self.drop_e = torch.nn.Dropout(args.drop_e)        self.drop_n = torch.nn.Dropout(args.drop_n)        e_in_dim = args.e_in_dim        n_in_dim = args.n_nodes        t_in_dim = args.t_in_dim        dim = args.n_dim        num_classes = args.n_classes        '''conv layer'''        self.conv1 = RHopConv(n_in_dim, e_in_dim, dim, self.act, args)        self.conv2 = RHopConv(dim, dim, dim, self.act, args)        '''output layer'''        if self.p.outputlayer == "gcn":            if self.p.res:                self.fc_node = NodeConv_att(n_in_channels=2 * dim, e_in_channels=2 * dim, out_channels=num_classes,                                            args=args, heads=args.heads)            else:                self.fc_node = NodeConv_att(n_in_channels=dim, e_in_channels=2 * dim, out_channels=num_classes,                                            args=args, heads=args.heads)        else:            if self.p.res:                self.fc_node = Linear(2 * dim, num_classes)            else:                self.fc_node = Linear(dim, num_classes)        '''type layer'''        if self.p.type_attr:            self.fc_type = Linear(t_in_dim, dim)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.fc_node.reset_parameters()        if self.p.type_attr:            self.fc_type.reset_parameters()    def forward(self, data):        edge_index, edge_type = data.edge_index[:, data.tr_edge_mask], data.edge_type[data.tr_edge_mask]        e0 = self.drop_e(data.edge_attr[data.tr_edge_mask])        if data.type_attr is not None:            t = self.drop_e(self.fc_type(data.type_attr))        else:            t = None        '''==========1/2-hop=========='''        x0, e = self.conv1(x=None, edge_index=edge_index, edge_attr=e0, type_attr=t, edge_type=edge_type)        x0 = self.drop_n(x0)        e = self.drop_e(e)        x, e = self.conv2(x=x0, edge_index=edge_index, edge_attr=e, type_attr=t, edge_type=edge_type)        x = self.drop_n(x)        e = self.drop_e(e)        '''skip-connection: Initial residual'''        if self.p.res:            x = torch.cat([x, x0], dim=-1)        out = self.fc_node(x)        return F.log_softmax(out, dim=1), e, x, t    def __repr__(self):        return self.__class__.__name__class Hop(torch.nn.Module):    def __init__(self, args):        super(Hop, self).__init__()        self.p = args        self.drop_e = torch.nn.Dropout(args.dropout)        self.drop_n = torch.nn.Dropout(args.dropout)        n_in_dim = args.n_nodes        e_in_dim = args.e_in_dim        t_in_dim = args.t_in_dim        dim = args.n_dim        num_classes = args.n_classes        act_n = str2act(args.act)        act_e = str2act(args.act)        '''Node-Edge-Node-Edge'''        self.NConv1 = NodeConv_hop(n_in_channels=n_in_dim, e_in_channels=e_in_dim, out_channels=dim, args=args,                                   act=act_n)        self.EConv1 = EdgeConv_hop(n_in_channels=dim, e_in_channels=e_in_dim, out_channels=dim, args=args, act=act_e)        if self.p.n_layer == 2 and self.p.outputlayer == 'fc':            self.NConv2 = NodeConv_hop(n_in_channels=dim, e_in_channels=dim, out_channels=dim, args=args, act=act_n)            if self.p.e_layer == 2:                self.EConv2 = EdgeConv_hop(n_in_channels=dim, e_in_channels=dim, out_channels=dim, args=args, act=act_e)        if self.p.outputlayer == "gcn":            self.fc_node = NodeConv_hop(n_in_channels=dim, e_in_channels=dim, out_channels=num_classes, args=args,                                        act=torch.nn.Identity())            if self.p.e_layer == 2:                self.fc_edge = EdgeConv_hop(n_in_channels=num_classes, e_in_channels=dim, out_channels=dim, args=args,                                            act=torch.nn.Identity())        else:            self.fc_node = Linear(dim, num_classes)            self.fc_edge = Linear(dim, dim)        '''Type_Net'''        self.type_attr = args.type_attr        if args.type_attr:            self.fc_type = Linear(t_in_dim, dim)    def reset_parameters(self):        self.NConv1.reset_parameters()        self.EConv1.reset_parameters()        if self.p.n_layer == 2 and self.p.outputlayer == 'fc':            self.NConv2.reset_parameters()            self.EConv2.reset_parameters()        self.fc_node.reset_parameters()        if self.p.e_layer == 2:            self.fc_edge.reset_parameters()        if self.type_attr:            self.fc_type.reset_parameters()    def norm_edge(self, edge_attr, edge_index):        out = torch.abs(edge_attr)        edge_index_d, edge_attr_d = to_undirected_edge_attr(edge_index, out)        edge_index_d, edge_attr_d = remove_self_loops(edge_index_d, edge_attr_d)        edge_index_d, edge_attr_d = add_self_loops_with_edge_attr(edge_index_d, edge_attr_d, num_nodes)        edge_index_d, edge_attr_d, perm = sort_edge_index(edge_index_d, edge_attr_d)        edge_attr_d = doubly_stochastic_normlization(edge_index_d, edge_attr_d)        num_edges = edge_index.size(1)        mask = perm < num_edges        out = edge_attr_d[mask]        return out    def forward(self, data):        edge_index, edge_type = data.edge_index[:, data.tr_edge_mask], data.edge_type[data.tr_edge_mask]        e0 = self.drop_e(data.edge_attr[data.tr_edge_mask])        if data.type_attr is not None:            t = self.drop_e(self.fc_type(data.type_attr))        else:            t = None        # 1.update x; 2.update e        x = self.NConv1(x=None, edge_index=edge_index, edge_attr=e0, edge_type=edge_type, type_attr=t)        x = self.drop_n(x)        e = self.EConv1(x=x, edge_index=edge_index, edge_attr=e0, edge_type=edge_type, type_attr=t,                        label_edge_idx=data.label_edge_idx)        e = self.drop_e(e)        if self.p.n_layer == 2 and self.p.outputlayer == 'fc':            x = self.NConv2(x=x, edge_index=edge_index, edge_attr=e, edge_type=edge_type, type_attr=t)            x = self.drop_n(x)            e = self.EConv2(x=x, edge_index=edge_index, edge_attr=e, edge_type=edge_type, type_attr=t,                            label_edge_idx=data.label_edge_idx)            e = self.drop_e(e)        '''fc-node'''        if self.p.outputlayer == 'gcn':            x = self.fc_node(x=x, edge_index=edge_index, edge_attr=e, type_attr=t, edge_type=edge_type)            if self.p.e_layer == 2:                e = self.fc_edge(x=x, edge_index=edge_index, edge_attr=e, type_attr=t, edge_type=edge_type,                                 label_edge_idx=data.label_edge_idx)            out = x        else:            out = self.fc_node(x)            e = self.fc_edge(e)        return F.log_softmax(out, dim=1), e, x, t    def __repr__(self):        return self.__class__.__name__