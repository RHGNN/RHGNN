import torchimport torch.nn.functional as Ffrom torch.nn import Linear, Sequentialfrom models.conv import GCNConv, GATConv, CompGCNConv, CompGCNConvBasis, RGCNConv, \ \NodeConv_ec, EdgeConv_Maskfrom utils.helper import *from torch.nn import Sequential as Seq, Linear as Lin, ReLUfrom torch_scatter import scatter_meanfrom torch.nn import Parameter as Paramdef str2act(str):    if str == 'elu':        act = torch.nn.ELU()    elif str == 'relu':        act = torch.nn.ReLU()    elif str == 'tanh':        act = torch.nn.Tanh()    elif str == 'identity':        act = torch.nn.Identity()    return actclass GCN(torch.nn.Module):    def __init__(self, args):        super(GCN, self).__init__()        self.p = args        self.in_dim = args.n_nodes        hidden = args.n_dim        num_classes = args.n_classes        self.drop = torch.nn.Dropout(args.dropout)        self.conv1 = GCNConv(self.in_dim, hidden)        self.conv2 = GCNConv(hidden, hidden)        if args.n_layer == 3:            self.conv3 = GCNConv(hidden, hidden)        self.fc = Linear(hidden, num_classes)        self.act = str2act(args.act)        self.reset_parameters()    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        if self.p.n_layer == 3:            self.conv3.reset_parameters()        self.fc.reset_parameters()    def forward(self, edge_index, edge_type):        x = self.act(self.conv1(x=None, edge_index=edge_index))        x = self.drop(x)        x = self.act(self.conv2(x, edge_index))        x = self.drop(x)        if self.p.n_layer == 3:            x = self.act(self.conv3(x, edge_index))        out = self.fc(x)        return F.log_softmax(out, dim=1)    def __repr__(self):        return self.__class__.__name__class GAT(torch.nn.Module):    def __init__(self, args):        super(GAT, self).__init__()        self.in_dim = args.n_nodes        hidden = args.n_dim        heads = args.heads        num_classes = args.n_classes        self.drop = torch.nn.Dropout(args.dropout)        self.init_embed = Param(torch.Tensor(args.n_nodes, args.n_in_dim))        self.conv1 = GATConv(args.n_in_dim, hidden, heads=heads, dropout=args.dropout)        self.conv2 = GATConv(hidden, hidden, heads=1, dropout=args.dropout)        self.fc = Linear(hidden, num_classes)        self.reset_parameters()    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.fc.reset_parameters()        torch.nn.init.xavier_normal_(self.init_embed)    def forward(self, edge_index, edge_type):        x = self.init_embed        x = F.elu(self.conv1(x=x, edge_index=edge_index))        x = self.drop(x)        x = F.elu(self.conv2(x, edge_index))        x = self.drop(x)        out = self.fc(x)        return F.log_softmax(out, dim=1)    def __repr__(self):        return self.__class__.__name__class RGCN(torch.nn.Module):    def __init__(self, args):        super(RGCN, self).__init__()        hidden = args.n_dim        num_classes = args.n_classes        num_relations = args.n_relations        num_bases = args.num_bases        self.init_embed = Param(torch.Tensor(args.n_nodes, args.n_in_dim))        self.drop = torch.nn.Dropout(args.dropout)        self.conv1 = RGCNConv(            args.n_in_dim, hidden, num_relations, num_bases=num_bases, num_nodes=args.n_nodes)        self.conv2 = RGCNConv(            hidden, hidden, num_relations, num_bases=num_bases, num_nodes=args.n_nodes)        self.fc = Linear(hidden, num_classes)        self.reset_parameters()    def reset_parameters(self):        torch.nn.init.xavier_normal_(self.init_embed)        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.fc.reset_parameters()    def forward(self, edge_index, edge_type):        # edge_index = data.edge_index        # edge_type = data.edge_type        x = self.drop(self.init_embed)        x = F.relu(self.conv1(x, edge_index, edge_type))        x = self.drop(x)        x = F.relu(self.conv2(x, edge_index, edge_type))        x = self.drop(x)        out = self.fc(x)        return F.log_softmax(out, dim=1)    def __repr__(self):        return self.__class__.__name__class CompGCN(torch.nn.Module):    def __init__(self, args):        super(CompGCN, self).__init__()        self.p = args        self.act = torch.tanh        # self.act = str2act(args.act)        n_in_dim = args.n_in_dim        n_dim = args.n_dim        num_rel = args.n_relations        num_classes = args.n_classes        self.init_embed = Param(torch.Tensor(args.n_nodes, n_in_dim))        if self.p.num_bases > 0:            self.init_rel = Param(torch.Tensor(self.p.num_bases, n_in_dim))            self.conv1 = CompGCNConvBasis(n_in_dim, n_dim, num_rel, self.p.num_bases, act=self.act,                                          params=self.p)        else:            self.init_rel = Param(torch.Tensor(num_rel, n_in_dim))            self.conv1 = CompGCNConv(n_in_dim, n_dim, num_rel, act=self.act, params=self.p)        self.conv2 = CompGCNConv(n_dim, n_dim, num_rel, act=torch.nn.Identity(),                                 params=self.p) if self.p.n_layer == 2 else None        self.drop = torch.nn.Dropout(self.p.dropout)        self.fc_e = Linear(n_dim, num_classes)        self.reset_parameters()    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.fc_e.reset_parameters()        torch.nn.init.xavier_normal_(self.init_embed)        torch.nn.init.xavier_normal_(self.init_rel)    def forward(self, edge_index, edge_type):        x = self.init_embed        r = self.init_rel        x, r = self.conv1(x, edge_index, edge_type, rel_embed=r)        x = self.drop(x)        x, r = self.conv2(x, edge_index, edge_type, rel_embed=r) if self.p.n_layer == 2 else (x, r)        x = self.drop(x) if self.p.n_layer == 2 else x        out = self.fc_e(x)        return F.log_softmax(out, dim=1)class MetaLayer(torch.nn.Module):    def __init__(self, args):        super(MetaLayer, self).__init__()        self.p = args        n_in_dim = args.n_in_dim        e_in_dim = args.n_in_dim        n_dim = args.n_dim        e_dim = args.n_dim        n_rels = args.n_rels        self.init_x = Param(torch.Tensor(args.n_nodes, n_in_dim))        self.init_e = Param(torch.Tensor(args.n_edges, e_in_dim))        self.fc1 = Lin(n_in_dim, n_dim)        self.fc2 = Lin(n_in_dim, e_dim)        self.edge_mlp = Seq(Lin(2 * n_dim + e_in_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        self.node_mlp_1 = Seq(Lin(n_in_dim + e_in_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        self.node_mlp_2 = Seq(Lin(n_in_dim + n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        self.edge_mlp2 = Seq(Lin(2 * n_dim + e_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        self.node_mlp2_1 = Seq(Lin(n_dim + e_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        self.node_mlp2_2 = Seq(Lin(2 * n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        if self.p.n_layer == 3:            self.node_mlp3_1 = Seq(Lin(n_dim + e_dim, n_dim), ReLU(), Lin(n_dim, n_dim))            self.node_mlp3_2 = Seq(Lin(2 * n_dim, n_dim), ReLU(), Lin(n_dim, n_dim))        if self.p.e_layer == 3:            self.edge_mlp3 = Seq(Lin(2 * n_dim + e_dim, e_dim), ReLU(), Lin(e_dim, e_dim))        self.fc_edge = Lin(n_dim, n_rels)        self.reset_parameters()    def reset_parameters(self):        torch.nn.init.xavier_normal_(self.init_x)        torch.nn.init.xavier_normal_(self.init_e)        self.fc_edge.reset_parameters()        for item in [self.edge_mlp, self.node_mlp_1, self.node_mlp_2,                     self.edge_mlp2, self.node_mlp2_1, self.node_mlp2_2]:            if hasattr(item, 'reset_parameters'):                item.reset_parameters()        if self.p.n_layer == 3:            for item in [self.node_mlp3_1, self.node_mlp3_2]:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()        if self.p.e_layer == 3:            for item in self.edge_mlp3:                if hasattr(item, 'reset_parameters'):                    item.reset_parameters()    def forward(self, edge_index, edge_type):        x = F.dropout(self.init_x, p=self.p.drop_n)        edge_attr = F.dropout(self.init_e, p=self.p.drop_e)        row, col = edge_index        "node layer"        x_msg = torch.cat([x[row], edge_attr], dim=1)        x_msg = self.node_mlp_1(x_msg)        x_msg = scatter_mean(x_msg, col, dim=0, dim_size=x.size(0))        x = torch.cat([x, x_msg], dim=1)        x = self.node_mlp_2(x)        x = F.dropout(x, p=self.p.drop_n)        "edge layer"        src = x[row]        dest = x[col]        edge_attr = torch.cat([src, dest, edge_attr], dim=1)        edge_attr = self.edge_mlp(edge_attr)        edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        "node layer 2"        x_msg = torch.cat([x[row], edge_attr], dim=1)        x_msg = self.node_mlp2_1(x_msg)        x_msg = scatter_mean(x_msg, col, dim=0, dim_size=x.size(0))        x = torch.cat([x, x_msg], dim=1)        x = self.node_mlp2_2(x)        x = F.dropout(x, p=self.p.drop_n)        "edge layer 2"        src = x[row]        dest = x[col]        edge_attr = torch.cat([src, dest, edge_attr], dim=1)        edge_attr = self.edge_mlp2(edge_attr)        edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        "node layer 3"        if self.p.n_layer == 3:            x_msg = torch.cat([x[row], edge_attr], dim=1)            x_msg = self.node_mlp3_1(x_msg)            x_msg = scatter_mean(x_msg, col, dim=0, dim_size=x.size(0))            x = torch.cat([x, x_msg], dim=1)            x = self.node_mlp3_2(x)            x = F.dropout(x, p=self.p.drop_n)        "edge layer 3"        if self.p.e_layer == 3:            src = x[row]            dest = x[col]            edge_attr = torch.cat([src, dest, edge_attr], dim=1)            edge_attr = self.edge_mlp3(edge_attr)            edge_attr = F.dropout(edge_attr, p=self.p.drop_e)        out = self.fc_edge(edge_attr)        return F.log_softmax(out, dim=1), xclass Hop(torch.nn.Module):    def __init__(self, args):        super(Hop, self).__init__()        self.p = args        n_in_dim = args.n_nodes        e_in_dim = args.n_edges        dim = args.n_dim        edge_dim = args.n_dim        n_rels = args.n_rels        self.x_init = Parameter(torch.Tensor(n_in_dim, dim))        self.e_init = Parameter(torch.Tensor(e_in_dim, dim))        '''Node_Net'''        nn1 = Sequential(Linear(dim + edge_dim, dim))        self.NConv1 = NodeConv_ec(dim, dim, nn1, aggr='add', root_weight=True)        self.NConv2 = NodeConv_ec(dim, dim, nn1, aggr='add', root_weight=True)        if self.p.n_layer >= 3:            self.NConv3 = NodeConv_ec(dim, dim, nn1, aggr='add', root_weight=True)        '''Edge_Net'''        nn2 = Sequential(Linear(2 * dim + edge_dim, edge_dim))        self.EConv1 = EdgeConv_Mask(edge_dim, edge_dim, nn2)        self.EConv2 = EdgeConv_Mask(edge_dim, edge_dim, nn2)        if self.p.e_layer >= 3:            self.EConv3 = EdgeConv_Mask(edge_dim, edge_dim, nn2)        self.fc_edge = Linear(edge_dim, n_rels)        self.reset_parameters()    def reset_parameters(self):        glorot(self.x_init)        glorot(self.e_init)        # self.fc1.reset_parameters()        # self.fc2.reset_parameters()        self.fc_edge.reset_parameters()        self.NConv1.reset_parameters()        self.NConv2.reset_parameters()        self.EConv1.reset_parameters()        self.EConv2.reset_parameters()        if self.p.n_layer == 3:            self.NConv3.reset_parameters()        if self.p.e_layer == 3:            self.EConv3.reset_parameters()    def forward(self, edge_index, edge_type):        x = F.dropout(self.x_init, p=self.p.dropout, training=False)        edge_attr = F.dropout(self.e_init, p=self.p.dropout, training=False)        type_attr = None        '''learn node embedding 1'''        x = torch.tanh(self.NConv1(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                                   edge_norm=None, edge_type=edge_type))        x = F.dropout(x, p=self.p.dropout, training=False)        '''learn edge embedding 1'''        edge_attr = torch.tanh(self.EConv1(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                                           edge_type=edge_type))        edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        '''learn node embedding 2'''        x = torch.tanh(self.NConv2(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                                   edge_norm=None, edge_type=edge_type))        x = F.dropout(x, p=self.p.dropout, training=False)        '''learn edge embedding 2'''        edge_attr = torch.tanh(self.EConv2(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                                           edge_type=edge_type))        edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        '''learn node embedding 3'''        if self.p.n_layer >= 3:            x = torch.tanh(self.NConv3(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                                       edge_norm=None, edge_type=edge_type))            x = F.dropout(x, p=self.p.dropout, training=False)        '''learn edge embedding 2'''        if self.p.e_layer >= 3:            edge_attr = torch.tanh(                self.EConv3(x=x, edge_index=edge_index, edge_attr=edge_attr, type_attr=type_attr,                            edge_type=edge_type))            edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        edge_attr = self.fc_edge(edge_attr)        edge_attr = F.dropout(edge_attr, p=self.p.dropout, training=False)        return F.log_softmax(edge_attr, dim=1), x