import torchfrom utils.train_eval import init_seeds, train, test, val, split_train_valfrom utils.process_graph import edge_sample, compute_node_loss_edgeimport torch.nn.functional as Fimport numpy as npdef one_train(args, data, model):    '''No validation'''    accs = torch.zeros(10)    counter = 0    for i in range(10):        init_seeds(args.seed)        model.reset_parameters()        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)        best_acc = 0        for epoch in range(1, args.epochs):            loss = train(model, optimizer, data, args, epoch)            (train_acc, test_acc), test_loss = test(model, data)            if epoch % 20 == 0:                if isinstance(loss, tuple):                    log = 'Epoch: {:03d}, Train Loss: {:.3f}, Test Loss: {:.3f}, Edge Reg: {:.2f}, Node Reg: {:.2f}, Train: {:.2f}, Test: {:.2f}'                else:                    log = 'Epoch: {:03d}, Train Loss: {:.3f}, Test Loss: {:.3f}, Train: {:.3f}, Test: {:.3f}'            if test_acc > best_acc:                best_acc = test_acc                high_train_acc = train_acc                counter = 0            else:                counter += 1                if counter >= args.patience and epoch > args.min_epochs:                    # logging.info("Early stopping")                    break        accs[i] = best_acc        print('Fold: {:02d}, Train: {:.4f}, Test: {:.4f}'.format(i, high_train_acc, best_acc))    print('Test Accuracy: {:.2f} ± {:.2f}'.format(accs.mean() * 100, accs.std() * 100))def ten_train(args, data, model, rel_data=None):    train_idx = data.train_idx    train_y = data.train_y    num_train = len(train_idx)    args.sp_ratio = 0.1    folds = int(1 / args.sp_ratio)    accs = torch.zeros(folds)    counter = 0    idx = torch.randperm(num_train)    split_train_val(data, train_idx, train_y, idx, 1, args.sp_ratio)    for i in range(1, folds + 1):        model.reset_parameters()        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)        best_val_acc = 0        best_acc = 0        for epoch in range(1, args.epochs):            loss = train(model, optimizer, data, args, epoch, rel_data)            (train_acc, val_acc, test_acc), val_loss, test_loss = val(model, data, args, rel_data)            if val_acc > best_val_acc:                best_acc = test_acc                best_val_acc = val_acc                cor_train_acc = train_acc                counter = 0            else:                counter += 1                if val_acc == best_val_acc:                    if test_acc > best_acc:                        best_acc = test_acc                if counter >= args.patience and epoch > args.min_epochs:                    # logging.info("Early stopping")                    break        if isinstance(loss, tuple):            log = 'Fold: {:02d}, Train Loss: {:.3f}, Test Loss: {:.3f}, Edge Reg: {:.2f}, Train: {:.2f}, Val: {:.2f}, Test: {:.2f}'            print(log.format(i, loss[0], test_loss, loss[1], cor_train_acc, best_val_acc * 100, best_acc * 100))        else:            log = 'Fold: {:02d}, Train Loss: {:.3f}, Val Loss:{:.3f}, Test Loss: {:.3f}, Train: {:.3f}, Val:{:.3f}, Test: {:.3f}'            print(log.format(i, loss, val_loss, test_loss, cor_train_acc, best_val_acc, best_acc))        accs[i - 1] = best_acc    acc = accs.mean() * 100    std = accs.std() * 100    print('Test Accuracy: {:.2f} ± {:.2f}'.format(acc, std))    return acc, stddef ten_train_one_fold(args, data, model, rel_data=None):    accs = torch.zeros(10)    counter = 0    length = len(data.train_idx)    data.val_idx = data.train_idx[:length // 5]    data.train_idx = data.train_idx[length // 5:]    data.val_y = data.train_y[:length // 5]    data.train_y = data.train_y[length // 5:]    seeds = [1233, 123, 12, 1, 111, 222, 333, 1234, 234, 444]    for i in range(1, 11):        init_seeds(seeds[i - 1])        model.reset_parameters()        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)        best_val_acc = 0        best_acc = 0        for epoch in range(1, args.epochs):            loss = train(model, optimizer, data, args, epoch, rel_data)            (train_acc, val_acc, test_acc), val_loss, test_loss = val(model, data, args, rel_data)            if val_acc > best_val_acc:                best_acc = test_acc                best_val_acc = val_acc                cor_train_acc = train_acc                counter = 0            else:                counter += 1                if val_acc == best_val_acc:                    if test_acc > best_acc:                        best_acc = test_acc                if counter >= args.patience and epoch > args.min_epochs:                    break        if isinstance(loss, tuple):            log = 'Fold: {:02d}, Train Loss: {:.3f}, Test Loss: {:.3f}, Edge Reg: {:.2f}, Train: {:.2f}, Val: {:.2f}, Test: {:.2f}'            print(log.format(i, loss[0], test_loss, loss[1], cor_train_acc, best_val_acc * 100, best_acc * 100))        else:            log = 'Fold: {:02d}, Train Loss: {:.3f}, Val Loss:{:.3f}, Test Loss: {:.3f}, Train: {:.2f}, Val:{:.2f}, Test: {:.2f}'            print(log.format(i, loss, val_loss, test_loss, cor_train_acc, best_val_acc * 100, best_acc * 100))        accs[i - 1] = best_acc    acc = accs.mean() * 100    std = accs.std() * 100    print('Test Accuracy: {:.2f} ± {:.2f}'.format(acc, std))    return acc, stdfrom utils.train_eval import train_ec, test_ecdef cross_folds_ec(args, data, model):    if args.model in ['Hop', 'Meta']:        num_trains = data.num_edges        train_y = data.edge_type    else:        num_trains = args.n_nodes        train_y = data.y    fold = int(num_trains * args.sp_ratio)    accs = torch.zeros(10)    micros = torch.zeros(10)    macros = torch.zeros(10)    for i in range(1, 11):        init_seeds(args.seed)        idx = torch.randperm(num_trains)        data.test_idx = idx[:fold]        data.test_y = train_y[data.test_idx]        data.train_idx = idx[fold:]        data.train_y = train_y[data.train_idx]        model.reset_parameters()        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=args.weight_decay)        best_acc = 0        high_micro = 0        high_macro = 0        counter = 0        if args.cuda is not None and int(args.cuda) >= 0:            data.test_idx = data.test_idx.to(args.device)            data.test_y = data.test_y.to(args.device)            data.train_idx = data.train_idx.to(args.device)            data.train_y = data.train_y.to(args.device)        for epoch in range(1, args.epochs):            model.train()            if args.model in ['Hop', 'Meta']:                edge_index = data.edge_index                edge_type = data.edge_type                if args.cuda is not None and int(args.cuda) >= 0:                    edge_index = edge_index.to(args.device)                    edge_type = edge_type.to(args.device)                optimizer.zero_grad()                out = model(edge_index, edge_type)                if isinstance(out, tuple):                    out, x_emb = out[0], out[1]                    loss = F.nll_loss(out[data.train_idx], data.train_y)                    node_reg = compute_node_loss_edge(x_emb[args.x_label_idx], args.x_label, args.n_classes)                    '''node type regularization'''                    loss = loss + node_reg * args.w_node_reg                else:                    loss = F.nll_loss(out[data.train_idx], data.train_y)                loss.backward()                optimizer.step()                train_loss = loss            else:                if args.batch_size == 0:                    edge_idx = torch.arange(data.num_edges)                    batch_size = data.num_edges + 1                else:                    edge_idx = torch.randperm(data.num_edges)                    batch_size = args.batch_size                steps = int(data.num_edges // batch_size) + 1                losses = []                for step in range(steps):                    l = step * batch_size                    r = l + batch_size                    if step == steps - 1:                        step_edge_idx = edge_idx[l:]                        edge_index = data.edge_index[:, step_edge_idx]                        edge_type = data.edge_type[step_edge_idx]                    else:                        step_edge_idx = edge_idx[l:r]                        edge_index = data.edge_index[:, step_edge_idx]                        edge_type = data.edge_type[step_edge_idx]                    if args.cuda is not None and int(args.cuda) >= 0:                        edge_index = edge_index.to(args.device)                        edge_type = edge_type.to(args.device)                    optimizer.zero_grad()                    out = model(edge_index, edge_type)                    if isinstance(out, tuple):                        out, x_emb = out[0], out[1]                        loss = F.nll_loss(out[data.train_idx], data.train_y)                        node_reg = compute_node_loss_edge(x_emb[args.x_label_idx], args.x_label, args.n_classes)                        '''node type regularization'''                        loss = loss + node_reg * args.w_node_reg                    else:                        loss = F.nll_loss(out[data.train_idx], data.train_y)                    loss.backward()                    optimizer.step()                    losses.append(loss.item())                train_loss = np.mean(losses)            (train_acc, test_acc), f1_micro, f1_macro = test_ec(model, edge_index, edge_type, data)            if test_acc > best_acc:                best_acc = test_acc                high_train_acc = train_acc                high_micro = f1_micro                high_macro = f1_macro                counter = 0            else:                counter += 1                if counter >= args.patience and epoch > args.min_epochs:                    break        accs[i - 1] = best_acc        micros[i - 1] = high_micro        macros[i - 1] = high_macro    return accs.mean() * 100, accs.std() * 100, micros.mean() * 100, micros.std() * 100