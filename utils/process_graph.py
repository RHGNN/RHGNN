import torchimport torch.distributions as tdistimport torch.nn.functional as Ffrom torch_sparse import coalesce, transpose, spspmmfrom torch_scatter import scatter_add, scatter_mean, scatter_maxfrom torch_geometric.data import Datafrom torch_geometric.datasets import Entitiesfrom torch_geometric.utils import degree, to_networkx, contains_self_loops, is_undirected, remove_self_loopsfrom torch_geometric.utils.num_nodes import maybe_num_nodesimport os.path as ospfrom sklearn.metrics import f1_scoreimport numpy as npdef degree_edge(edge_attr, index, num_nodes=None, dtype=None):    """Computes the degree of a given index tensor.    Args:        index (LongTensor): Source or target indices of edges.        num_nodes (int, optional): The number of nodes in :attr:`index`.            (default: :obj:`None`)        dtype (:obj:`torch.dtype`, optional): The desired data type of the            returned tensor.    :rtype: :class:`Tensor`    .. testsetup::        import torch    .. testcode::        from torch_geometric.utils import degree        index = torch.tensor([0, 1, 0, 2, 0])        out = degree(index)    """    num_nodes = maybe_num_nodes(index, num_nodes)    out = scatter_add(edge_attr, index, dim=0, dim_size=num_nodes)    # dim = edge_attr.size(-1)    # out = torch.zeros((num_nodes, dim), dtype=dtype, device=index.device)    # index = torch.unsqueeze(index, 1)    # scatter_add allows index tensor that doesn't match input size in forward pass but fails on backward pass    # return out.scatter_add_(0, index, edge_attr)    return outdef filter_adj(row, col, edge_attr, edge_type, mask):    return row[mask], col[mask], \ \    None if edge_attr is None else edge_attr[mask], \ \    None if edge_type is None else edge_type[mask]def dropout_adj(edge_index, edge_attr=None, edge_type=None, p=0.5, force_undirected=False,                num_nodes=None, training=True):    r"""Randomly drops edges from the adjacency matrix    :obj:`(edge_index, edge_attr)` with probability :obj:`p` using samples from    a Bernoulli distribution.    Args:        edge_index (LongTensor): The edge indices.        edge_attr (Tensor, optional): Edge weights or multi-dimensional            edge features. (default: :obj:`None`)        p (float, optional): Dropout probability. (default: :obj:`0.5`)        force_undirected (bool, optional): If set to :obj:`True`, will either            drop or keep both edges of an undirected edge.            (default: :obj:`False`)        num_nodes (int, optional): The number of nodes, *i.e.*            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)        training (bool, optional): If set to :obj:`False`, this operation is a            no-op. (default: :obj:`True`)    """    if p < 0. or p > 1.:        raise ValueError('Dropout probability has to be between 0 and 1, '                         'but got {}'.format(p))    mask = torch.ones(edge_type.size(0), dtype=torch.bool)    if not training:        return edge_index, edge_attr, edge_type, mask    if p == 0.:        return edge_index, edge_attr, edge_type, mask    N = maybe_num_nodes(edge_index, num_nodes)    row, col = edge_index    # random sampling the neighbor nodes    # seed = int(time.time() * 256)    # torch.manual_seed(seed)    if force_undirected:        row, col, edge_attr, edge_type = filter_adj(row, col, edge_attr, edge_type, row < col)    mask = edge_index.new_full((row.size(0),), 1 - p, dtype=torch.float)    mask = torch.bernoulli(mask).to(torch.bool)    row, col, edge_attr, edge_type = filter_adj(row, col, edge_attr, edge_type, mask)    if force_undirected:        edge_index = torch.stack(            [torch.cat([row, col], dim=0),             torch.cat([col, row], dim=0)], dim=0)        if edge_attr is not None:            edge_attr = torch.cat([edge_attr, edge_attr], dim=0)        if edge_type is not None:            edge_type = torch.cat([edge_type, edge_type], dim=0)        _, edge_type = coalesce(edge_index, edge_type, N, N, 'max')        edge_index, edge_attr = coalesce(edge_index, edge_attr, N, N, 'add')    else:        edge_index = torch.stack([row, col], dim=0)    return edge_index, edge_attr, edge_type, maskdef to_undirected_edge_attr(edge_index, edge_attr, num_nodes=None):    num_nodes = maybe_num_nodes(edge_index, num_nodes)    row, col = edge_index    row, col = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)    edge_index = torch.stack([row, col], dim=0)    edge_attr = torch.cat([edge_attr, edge_attr], dim=0)    edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes, num_nodes)    return edge_index, edge_attrdef add_self_loops_with_edge_attr(edge_index, edge_attr, num_nodes=None):    dtype, device = edge_index.dtype, edge_index.device    loop = torch.arange(0, num_nodes, dtype=dtype, device=device)    loop = loop.unsqueeze(0).repeat(2, 1)    edge_index = torch.cat([edge_index, loop], dim=1)    ones = torch.ones([edge_index.shape[1] - edge_attr.shape[0], edge_attr.shape[1]], dtype=edge_attr.dtype,                      device=edge_attr.device)    edge_attr = torch.cat([edge_attr, ones], dim=0)    assert edge_index.shape[1] == edge_attr.shape[0]    return edge_index, edge_attrdef doubly_stochastic_normlization(edge_index, edge_attr, num_nodes=None):    eps = 1e-5    row, col = edge_index    num_nodes = maybe_num_nodes(edge_index, num_nodes)    "Eq.(1)"    row_norm = scatter_add(edge_attr, row, dim=0, dim_size=num_nodes)[row] + eps    tilde_edge = edge_attr * (1. / row_norm)    "Eq.(2)"    denom = scatter_add(tilde_edge, col, dim=0, dim_size=num_nodes)[col] + eps    tilde_edge = tilde_edge * (1. / torch.sqrt(denom))    edge_index_t, tilde_edge_t = transpose(edge_index, tilde_edge, num_nodes, num_nodes)    for dim in range(0, tilde_edge.size(1)):        indexC, valueC = spspmm(indexA=edge_index, valueA=tilde_edge[:, dim],                                indexB=edge_index_t, valueB=tilde_edge_t[:, dim],                                m=num_nodes, k=num_nodes, n=num_nodes)        row_c, col_c = indexC        src = torch.sparse.FloatTensor(edge_index, 2 * valueC.new_ones(row.size(0)), torch.Size([num_nodes, num_nodes]))        des = torch.sparse.FloatTensor(indexC, valueC.new_ones(row_c.size(0)), torch.Size([num_nodes, num_nodes]))        res = src - des        "[src - des]=mask: -1=increased edge, 1=original edge, 2=reduced edge"        mask = res._values() != -1  # existing edge_attr        mask_red = res._values() == 2  # reduced edge        mask_up = res._values() == 1  # update edge        base_attr = res._values()        # Retain original edge attr for reduced edge        src = torch.sparse.FloatTensor(edge_index, 2 * valueC.new_ones(row.size(0)) + edge_attr[:, dim],                                       torch.Size([num_nodes, num_nodes]))        des = torch.sparse.FloatTensor(indexC, valueC.new_ones(row_c.size(0)), torch.Size([num_nodes, num_nodes]))        res = src - des        base_attr[mask_red] = res._values()[mask_red] - 2        # Update edge attr for existing edge        src = torch.sparse.FloatTensor(edge_index, 2 * valueC.new_ones(row.size(0)), torch.Size([num_nodes, num_nodes]))        des = torch.sparse.FloatTensor(indexC, valueC.new_ones(row_c.size(0)) - valueC,                                       torch.Size([num_nodes, num_nodes]))        res = src - des        base_attr[mask_up] = res._values()[mask_up] - 1        edge_attr[:, dim] = base_attr[mask]    row_sum = scatter_add(edge_attr, row, dim=0, dim_size=num_nodes)    col_sum = scatter_add(edge_attr, col, dim=0, dim_size=num_nodes)    return edge_attrdef sort_edge_index(edge_index, edge_attr=None, num_nodes=None):    r"""Row-wise sorts edge indices :obj:`edge_index`.    Args:        edge_index (LongTensor): The edge indices.        edge_attr (Tensor, optional): Edge weights or multi-dimensional            edge features. (default: :obj:`None`)        num_nodes (int, optional): The number of nodes, *i.e.*            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)    :rtype: (:class:`LongTensor`, :class:`Tensor`)    """    num_nodes = maybe_num_nodes(edge_index, num_nodes)    idx = edge_index[0] * num_nodes + edge_index[1]    perm = idx.argsort()    return edge_index[:, perm], None if edge_attr is None else edge_attr[perm], permdef load_data(name):    path = osp.join(        osp.dirname(osp.realpath(__file__)), '..', 'data', 'Entities', name)    dataset = Entities(path, name)    return datasetdef init_data(data, args, num_rel):    "Init node"    build_x(data, args.dataset)    "Init type"    if args.type_attr:        data.type_attr = init_type(args.dataset, num_rel)    else:        data.type_attr = torch.eye(num_rel)    "Init edge"    if args.dropedge > 0:        init_edge_attr(data, num_rel)    else:        label_edge_idx, unlabel_edge_idx = get_label_edge(data.num_edges, args.label_edge_rate)        init_edge_attr(data, num_rel, label_edge_idx, unlabel_edge_idx)  # init based on edge-type        data.label_edge_idx = label_edge_idx        data.unlabel_edge_idx = unlabel_edge_idx        data.tr_edge_mask = torch.ones(data.edge_type.size(-1), dtype=torch.bool)def build_x(data, name):    num_nodes = data.num_nodes    if name == 'AM' or name == 'BGS':        data.x = torch.sparse_coo_tensor(torch.arange(0, num_nodes).repeat(2, 1), torch.ones(num_nodes))    else:        data.x = torch.eye(num_nodes)def one_hot_degree(data):    row, x = data.edge_index[0], data.x    deg = degree(row, data.num_nodes, dtype=torch.long)    max_degree = int(torch.max(deg))    deg = F.one_hot(deg, num_classes=max_degree + 1).to(torch.float)    if data.x is not None:        x = x.view(-1, 1) if x.dim() == 1 else x        data.x = torch.cat([x, deg.to(x.dtype)], dim=-1)    else:        data.x = degdef init_type(name, num_rel):    type_attr = []    path = osp.join(        osp.dirname(osp.realpath(__file__)), '..', 'type_pretrain', name)    type_emb = torch.load(path + '_type_emb_0310_128_halfedge_0_walklength20')    if num_rel == type_emb.size(0):        for i in range(0, type_emb.size(0), 1):            type_attr.append(type_emb[i].tolist())    else:        for i in range(0, type_emb.size(0), 2):            type_attr.append((1 / 2 * (type_emb[i] + type_emb[i + 1])).tolist())    return torch.tensor(type_attr)def init_type_lp(name, num_rel):    type_attr = []    path = osp.join(        osp.dirname(osp.realpath(__file__)), '..', 'type_pretrain', name)    type_emb = torch.load(path + '_type_emb_0310_128_halfedge_0_walklength20')    for i in range(0, num_rel):        type_attr.append((1 / 2 * (type_emb[i] + type_emb[i + num_rel])).tolist())    return torch.tensor(type_attr)def init_edge_attr(data, num_relations, label_idx=None, unlabel_idx=None):    '''init using type'''    type = data.type_attr  # using type_attr    if label_idx is None:        data.edge_attr = F.embedding(data.edge_type, type)    else:        data.edge_attr = torch.zeros(data.num_edges, type.size(1))        if label_idx.size(0) > 0:            data.edge_attr[label_idx] = F.embedding(data.edge_type[label_idx], type)            if unlabel_idx.size(0) > 0:                mean_type = torch.mean(type, dim=0)                data.edge_attr[unlabel_idx] = mean_type        else:            data.edge_attr = torch.zeros(data.num_edges, num_relations)            torch.nn.init.xavier_uniform_(data.edge_attr, gain=1)def get_label_edge(num_edges, fold):    fold = int(num_edges * fold)    idx = torch.randperm(num_edges)    train_idx = idx[:fold]    test_idx = idx[fold:]    return train_idx, test_idxclass LineGraph(object):    r"""Converts a graph to its corresponding line-graph:    .. math::        L(\mathcal{G}) &= (\mathcal{V}^{\prime}, \mathcal{E}^{\prime})        \mathcal{V}^{\prime} &= \mathcal{E}        \mathcal{E}^{\prime} &= \{ (e_1, e_2) : e_1 \cap e_2 \neq \emptyset \}    Line-graph node indices are equal to indices in the original graph's    coalesced :obj:`edge_index`.    For undirected graphs, the maximum line-graph node index is    :obj:`(data.edge_index.size(1) // 2) - 1`.    New node features are given by history edge attributes.    For undirected graphs, edge attributes for reciprocal edges    :obj:`(row, col)` and :obj:`(col, row)` get summed together.    Args:        force_directed (bool, optional): If set to :obj:`True`, the graph will            be always treated as a directed graph. (default: :obj:`False`)    """    def __init__(self, force_directed=False):        self.force_directed = force_directed    def __call__(self, data):        N = data.num_nodes        edge_index, edge_attr = data.edge_index, data.edge_attr        row, col = edge_index        if self.force_directed:            joints = col            i = torch.arange(row.size(0), dtype=torch.long, device=row.device)            count = scatter_add(                torch.ones_like(row), row, dim=0, dim_size=data.num_nodes)            cumsum = torch.cat([count.new_zeros(1), count.cumsum(0)], dim=0)            cols = [                i[cumsum[col[j]]:cumsum[col[j] + 1]]                for j in range(col.size(0))            ]            rows = [row.new_full((c.numel(),), j) for j, c in enumerate(cols)]            row, col = torch.cat(rows, dim=0), torch.cat(cols, dim=0)            edge_index = torch.stack([row, col], dim=0)            x = data.edge_attr            '''edge_type'''            edge_type = [torch.LongTensor([joints[i] for _ in range(len(cols[i]))]) for i in range(len(joints))]            edge_type = torch.cat(edge_type, dim=0)            ''''''            line_data = Data(edge_index=edge_index, x=x)            line_data.__setitem__('edge_type', edge_type)        else:            # Compute node indices.            mask = row < col            row, col = row[mask], col[mask]            i = torch.arange(row.size(0), dtype=torch.long, device=row.device)            '''process multi-edge'''            row, indices = torch.sort(torch.cat([row, col], dim=0), dim=-1, descending=False)            i = torch.cat([i, i], dim=0)[indices]            count = scatter_add(                torch.ones_like(row), row, dim=0, dim_size=data.num_nodes)            joints = torch.split(i, count.tolist())            edge_type_len = []            def generate_grid(x):                row = x.view(-1, 1).repeat(1, x.numel()).view(-1)                col = x.repeat(x.numel())                mask = row < col                row, col = row[mask], col[mask]                edge_type_len.append(len(row))                return torch.stack([row, col], dim=0)            joints = list(joints)            for j in range(len(joints)):                joints[j] = generate_grid(joints[j])            joints = torch.cat(joints, dim=1)            edge_type = [torch.LongTensor([x for _ in range(edge_type_len[x])]) for x in range(len(edge_type_len))]            edge_type = torch.cat(edge_type, dim=0)            joints, edge_type = remove_self_loops(joints,                                                  edge_type)  # joints = [[0,0,1,1,2,2,1,3,2,3],[1,2,0,2,0,3,1,3,2]], edge_type=[0,0,0,0,0,0,2,2,3,3]            joints[0], indices = torch.sort(joints[0], dim=0, descending=False)            joints[1] = joints[1][indices]  # joints = [[0,0,1,1,1,2,2,2,3,3],[1,2,0,2,3,0,1,3,1,2]] = edge_index            edge_type = edge_type[indices]  # edge_type=[0,0,0,0,2,0,0,3,2,3]            if edge_attr is not None:                num_line_x = joints[0].max().item() + 1                x = scatter_add(edge_attr, i, dim=0, dim_size=num_line_x)            else:                x = None            line_data = Data(edge_index=joints, x=x, y=data.edge_type[mask])            line_data.__setitem__('edge_type', edge_type)            line_data.num_nodes = data.edge_type[mask].size(0)        return line_data    def __repr__(self):        return '{}()'.format(self.__class__.__name__)def convert_to_line_graph(force_directed, data):    line_graph = LineGraph(force_directed)(data)    return line_graphdef get_statistics_of_graph(graph):    if graph.contains_isolated_nodes():        print('graph contains isolated nodes!!')  # true    if graph.is_directed():        print('graph is directed!')  # true    if graph.contains_self_loops():        print('graph contains self loops!')  # false    if graph.is_coalesced():        print('graph is_coalesced!')  # truedef graph_property(data):    print('contains_self_loops: ', contains_self_loops(data.edge_index))  # False    print('is_undirected: ', is_undirected(data.edge_index))    print('num_edges: ', data.num_edges)    row, col = data.edge_index    mask = row > col    print(mask.sum().item())    mask = row < col    print(mask.sum().item())    num_relations = torch.unique(data.edge_type).size(0)    print('num_relations: ', num_relations)def normal_disturp():    mean = torch.tensor([4.0, 3.0])    sigma = torch.tensor([1.0, 1.0])    n = tdist.Normal(mean, sigma)    disturp = torch.tensor([3.1, 4.1])    disturp = n.icdf(disturp)def get_isolated_edge(edge_index, num_nodes):    row, col = edge_index    count_r = scatter_add(        torch.ones_like(row), row, dim=0, dim_size=num_nodes)    index_r = torch.ne(count_r, 1).nonzero()    mask_c = torch.eq(col, index_r[0])    for i in range(1, len(index_r)):        mask_c = mask_c + torch.eq(col, index_r[i])    index_cor = torch.masked_select(row, mask_c)    index = torch.cat([index_cor, index_r.squeeze(1)], dim=0)    index = torch.unique(index, sorted=True, return_inverse=False)    node_id = torch.arange(start=0, end=num_nodes, step=1)    mask = mask_c.new_ones(num_nodes)    for i in range(0, len(index)):        mask = mask - torch.eq(node_id, index[i])    isolated_edge = torch.masked_select(node_id, mask)    return isolated_edgedef remove_isolated_edge(data, isolated_edge=None):    row, col = data.edge_index    G = to_networkx(torch.stack([row, col], dim=0))    G.remove_edges_from()  # networkx    return datadef remove_duplicate_edge(data):    N = data.num_nodes    row, col = data.edge_index    type = data.edge_type    # num_clusters = torch.max(type) + 1    (new_row, new_col), type = coalesce(data.edge_index, type, N, N, op='max')    if len(new_row) < len(row):        print('Exist multi-edges in graph:', len(row) - len(new_row))        data.edge_index = torch.stack([new_row, new_col], dim=0)        data.edge_type = type        if data.edge_norm is not None:            _, inv = torch.unique(row * N + col, sorted=True, return_inverse=True)            scatter_add(data.edge_norm, inv, dim=-1, out=None, dim_size=None, fill_value=0)    # num_clusters = torch.max(type) + 1    returndef get_same_edge(edge_index):    row, col = edge_index    mask = row < col    row, col = row[mask], col[mask]    i = torch.arange(row.size(0), dtype=torch.long, device=row.device)    row, indices = torch.sort(torch.cat([row, col], dim=0), dim=-1, descending=False)    i = torch.cat([i, i], dim=0)[indices]    return idef copy_edge_attr(edge_index, edge_attr, line_x, num_nodes):    same_edge = get_same_edge(edge_index, num_nodes)    for i in range(len(same_edge)):        edge_attr[i] = line_x[same_edge[i]]    return edge_attrdef get_edge_center(edge_embed, edge_type):    num_types = (torch.max(edge_type) + 1)    center = scatter_mean(edge_embed, edge_type, dim=0, dim_size=num_types)    return centerdef compute_edge_loss(edge_emb, edge_type, type_attr=None, t_w=None):    # num_types = (torch.max(edge_type) + 1)    num_types = torch.unique(edge_type).size(0)    if type_attr is None:        center = get_edge_center(edge_emb, edge_type)    else:        center = type_attr    loss = 0    pdist = torch.nn.PairwiseDistance(p=2)    for i in range(num_types):        mask = torch.eq(edge_type, i)        if len(mask.nonzero()) > 0:            output = pdist(center[i].view(1, -1), edge_emb[mask])            # loss = loss + t_w[i] * torch.mean(output, dim=-1)            loss = loss + torch.mean(output, dim=-1)    return loss / num_types    # return lossdef reg_loss(embed, type):    types = torch.unique(type, sorted=True)    center = get_edge_center(embed, type)    loss = 0    # num_type = 0    pdist = torch.nn.PairwiseDistance(2)    i = 0    for t in types:        mask = torch.eq(type, t)        if len(mask.nonzero()) > 0:            output = pdist(center[i].view(1, -1), embed[mask])            loss = loss + torch.mean(output, dim=-1)        i = i + 1    # return loss / num_type    return lossdef compute_node_loss_edge(x_embed, x_type, num_classes):    center = scatter_mean(x_embed, x_type, dim=0, dim_size=num_classes)    num_type = len(center)    loss = 0    pdist = torch.nn.PairwiseDistance(2)    for i in range(len(center)):        mask = torch.eq(x_type, i)        if len(mask.nonzero()) > 0:            output = pdist(center[i].view(1, -1), x_embed[mask])            loss = loss + torch.mean(output, dim=-1)    # return loss / num_type    return lossdef convert_edge_type(data):    num_relations = torch.unique(data.edge_type).size(0)    for i in range(0, num_relations, 2):        mask = torch.eq(data.edge_type, i)        data.edge_type[mask] = int(i / 2)        if len(mask.nonzero()) > 0:            # find the correspond type            # row_i = row[mask][0]            # col_i = col[mask][0]            # mask_cor = torch.eq(row, col_i) * torch.eq(col, row_i)            # type_cor = data.edge_type[mask_cor]            # if len(type_cor) > 1:            type_cor = i + 1            mask_cor = torch.eq(data.edge_type, type_cor)            data.edge_type[mask_cor] = int(i / 2)        else:            print('len(mask)=0')    returndef half_edge(data):    row, col = data.edge_index    mask = row < col    edge_index = torch.stack([row[mask], col[mask]], dim=0)    data.edge_index = edge_index    data.edge_type = data.edge_type[mask]    if data.edge_attr is not None:        data.edge_attr = data.edge_attr[mask]    returndef to_undirected(edge_index, edge_type, num_nodes=None):    num_nodes = maybe_num_nodes(edge_index, num_nodes)    row, col = edge_index    row, col = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)    edge_index = torch.stack([row, col], dim=0)    edge_type = torch.cat([edge_type, edge_type], dim=0)    edge_index, edge_type = coalesce(edge_index, edge_type, num_nodes, num_nodes, 'max')    return edge_index, edge_typedef get_neighbor_edge(edge_index):    num_edges = edge_index.size(1)    row, col = edge_index    neighbor_edges = []    for i in range(0, num_edges):        src = row[i]        des = col[i]        mask = torch.eq(row, src) + torch.eq(col, src) + torch.eq(row, des) + torch.eq(col, des)        neighbor_edges.append(mask.nonzero().view(-1))    save(neighbor_edges, 'MUTAG_neigh_edges')    return neighbor_edgesdef save(val, file):    torch.save(val, file)    print('save successfully:' + file)def load(file):    val = torch.load(file)    return valdef get_dataset(name):    path = osp.join(        osp.dirname(osp.realpath(__file__)), '..', 'data', 'Entities', name)    dataset = Entities(path, name)    return datasetdef cross_folds_edge(data, split, fold):    num_edges = data.num_edges    edges_idx = torch.arange(num_edges, dtype=torch.long, device=data.x.device)    if split == 10:        r_sp = num_edges    else:        r_sp = split * fold    l_sp = r_sp - fold    test_mask = torch.zeros(num_edges, dtype=torch.long)    test_mask[l_sp:r_sp] = 1    data.test_idx = edges_idx[test_mask.eq(1)]    data.test_y = data.edge_type[test_mask.eq(1)]    data.train_idx = edges_idx[test_mask.eq(0)]    data.train_y = data.edge_type[test_mask.eq(0)]    returndef build_edge_attr_for_edge(data, dim, num_relations):    data.edge_attr = torch.Tensor(data.num_edges, dim)    torch.nn.init.xavier_uniform_(data.edge_attr, gain=1)def calc_f1(y_true, y_pred):    y_pred[y_pred > 0.5] = 1    y_pred[y_pred <= 0.5] = 0    return f1_score(y_true, y_pred, average="micro"), f1_score(y_true, y_pred, average="macro")'''===========statistic========='''def type_stats(types):    num_items = types.size(0)    num_rels = torch.unique(types).size(0)    sum = torch.zeros(num_rels)    for i in range(num_rels):        mask = torch.eq(types, i)        sum[i] = len(mask.nonzero()) / num_items    # mean, std = sum.mean(), sum.std()    return sumdef inverse_type_w(types):    types = 1. / types    types = F.normalize(types, p=1., dim=-1)    return typesdef softmax(src, index, num_nodes=None):    r"""Sparse softmax of all values from the :attr:`src` tensor at the indices    specified in the :attr:`index` tensor along the first dimension.    Args:        src (Tensor): The source tensor.        index (LongTensor): The indices of elements for applying the softmax.        num_nodes (int, optional): Automatically create output tensor with size            :attr:`num_nodes` in the first dimension. If set to :attr:`None`, a            minimal sized output tensor is returned. (default: :obj:`None`)    :rtype: :class:`Tensor`    .. testsetup::        import torch    .. testcode::        from torch_geometric.utils import softmax        src = torch.Tensor([2, 3, -2, 1, 1])        index = torch.tensor([0, 1, 0, 1, 2])        out = softmax(src, index)    """    num_nodes = maybe_num_nodes(index, num_nodes)    out = src - scatter_max(src, index, dim=0, dim_size=num_nodes)[0][index]    out = out.exp()    out = out / scatter_add(out, index, dim=0, dim_size=num_nodes)[index]    return outdef keep_sub_hop_graph(data, hop=2):    from torch_geometric.utils import k_hop_subgraph    node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)    node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(node_idx, hop, data.edge_index, relabel_nodes=True)    data.num_nodes = node_idx.size(0)  # server req.    data.edge_index = edge_index    data.edge_type = data.edge_type[edge_mask]    data.train_idx = mapping[:data.train_idx.size(0)]    data.test_idx = mapping[data.train_idx.size(0):]    returndef update_edge_attr(data, label_idx, unlabel_idx):    if unlabel_idx.size(0) > 0:        type = data.type_attr        mean_type = torch.mean(type, dim=0)        data.edge_attr[unlabel_idx] = mean_typedef edge_sample(data, args):    edge_index, edge_attr, edge_type, mask = dropout_adj(data.edge_index,                                                         edge_attr=data.edge_attr,                                                         edge_type=data.edge_type,                                                         p=args.dropedge,                                                         num_nodes=data.num_nodes)    num_edges = len(edge_type)    label_edge_idx, unlabel_edge_idx = get_label_edge(num_edges, args.label_edge_rate)    update_edge_attr(data, label_edge_idx, unlabel_edge_idx)    data.tr_edge_mask = mask    data.label_edge_idx = label_edge_idx    data.unlabel_edge_idx = unlabel_edge_idxdef add_self_edge_attr_loops(edge_attr, num_nodes=None):    dtype, device = edge_attr.dtype, edge_attr.device    loop = torch.ones(num_nodes, dtype=dtype, device=device)    edge_attr = torch.cat([edge_attr, loop], dim=0)    return edge_attrdef load_rel_graph(name):    name = name    x_path = '../data_preprocess/' + name + '-relation/rel_graph_x.npy'    edge_index_path = '../data_preprocess/' + name + '-relation/rel_graph_edge_index.npy'    edge_attr_path = '../data_preprocess/' + name + '-relation/rel_graph_edge_attr.npy'    rel_graph = Data()    rel_graph.x = torch.FloatTensor(np.load(x_path))    rel_graph.edge_index = torch.LongTensor(np.load(edge_index_path))    rel_graph.edge_attr = torch.FloatTensor(np.load(edge_attr_path))    return rel_graph