import torch.nn.functional as Ffrom utils.process_graph import compute_edge_loss, calc_f1, compute_node_loss_edgeimport osimport randomimport torchimport numpy as npdef init_seeds(seed=1233):    random.seed(seed)    os.environ['PYTHONHASHSEED'] = str(seed)    torch.manual_seed(seed)    np.random.seed(seed)    torch.cuda.manual_seed(seed)    torch.cuda.manual_seed_all(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = Falsedef train(model, optimizer, data, args, epoch, rel_data):    model.train()    optimizer.zero_grad()    if rel_data is not None:        out = model(data, rel_data)    else:        out = model(data)    if isinstance(out, tuple):        pred, edge_emb = out[0], out[1]        node_loss = F.nll_loss(pred[data.train_idx], data.train_y)        edge_reg = compute_edge_loss(edge_emb[data.label_edge_idx], data.edge_type[data.label_edge_idx])        loss = node_loss + edge_reg * args.w_edge_reg        loss.backward()        optimizer.step()        return loss, node_loss, edge_reg    else:        loss = F.nll_loss(out[data.train_idx], data.train_y)        loss.backward()        optimizer.step()        return lossdef test(model, data):    model.eval()    out = model(data)    if isinstance(out, tuple):        out = out[0]    accs = []    pred = out[data.train_idx].max(1)[1]    acc = pred.eq(data.train_y).sum().item() / data.train_y.size(0)    accs.append(acc)    pred = out[data.test_idx].max(1)[1]    acc = pred.eq(data.test_y).sum().item() / data.test_y.size(0)    accs.append(acc)    test_loss = F.nll_loss(out[data.test_idx], data.test_y)    return accs, test_lossdef val(model, data, args, rel_data):    model.eval()    if rel_data is not None:        out = model(data, rel_data)    else:        out = model(data)    if isinstance(out, tuple):        out = out[0]    accs = []    pred = out[data.train_idx].max(1)[1]    acc = pred.eq(data.train_y).sum().item() / data.train_y.size(0)    accs.append(acc)    # val    pred = out[data.val_idx].max(1)[1]    acc = pred.eq(data.val_y).sum().item() / data.val_y.size(0)    accs.append(acc)    val_loss = F.nll_loss(out[data.val_idx], data.val_y)    # test    pred = out[data.test_idx].max(1)[1]    acc = pred.eq(data.test_y).sum().item() / data.test_y.size(0)    accs.append(acc)    test_loss = F.nll_loss(out[data.test_idx], data.test_y)    return accs, val_loss, test_lossdef cross_folds(data, tr_idx, tr_y, ratio):    idx = torch.randperm(tr_idx.size(0))    fold = int(len(tr_idx) * ratio)    val_mask = idx[:fold]    tr_mask = idx[fold:]    data.val_idx = tr_idx[val_mask]    data.val_y = tr_y[val_mask]    data.train_idx = tr_idx[tr_mask]    data.train_y = tr_y[tr_mask]    returndef split_train_val(data, train_idx, train_y, idx, split, ratio):    # idx = torch.randperm(len(train_idx))    fold = int(len(train_idx) * ratio)    if split == int(1 / ratio):        r_sp = len(train_idx)    else:        r_sp = split * fold    l_sp = r_sp - fold    mask = torch.zeros(len(train_idx), dtype=torch.long)    mask[l_sp:r_sp] = 1    val_mask = idx[mask.eq(1)]    tr_mask = idx[mask.eq(0)]    data.val_idx = train_idx[val_mask]    data.val_y = train_y[val_mask]    data.train_idx = train_idx[tr_mask]    data.train_y = train_y[tr_mask]    returndef split_train_val_one(data, ratio):    tr_idx, tr_y = data.train_idx, data.train_y    fold = int(len(tr_idx) * ratio)    data.val_idx = tr_idx[:fold]    data.val_y = tr_y[:fold]    data.train_idx = tr_idx[fold:]    data.train_y = tr_y[fold:]    returndef train_ec(model, optimizer, edge_index, edge_type, data, args, epoch):    model.train()    if epoch == 100:        for param_group in optimizer.param_groups:            param_group['lr'] = 0.8 * param_group['lr']    optimizer.zero_grad()    out = model(edge_index, edge_type, )    if isinstance(out, tuple):        out, x_emb = out[0], out[1]        loss = F.nll_loss(out[data.train_idx], data.train_y)        node_reg = compute_node_loss_edge(x_emb[args.x_label_idx], args.x_label, args.n_n_classes)        loss = loss + node_reg * args.w_node_reg    else:        loss = F.nll_loss(out[data.train_idx], data.train_y)    loss.backward()    optimizer.step()    return lossdef test_ec(model, edge_index, edge_type, data):    model.eval()    out = model(edge_index, edge_type)    if isinstance(out, tuple):        out = out[0]    accs = []    pred = out[data.train_idx].max(1)[1]    acc = pred.eq(data.train_y).sum().item() / data.train_y.size(0)    accs.append(acc)    pred = out[data.test_idx].max(1)[1]    acc = pred.eq(data.test_y).sum().item() / data.test_y.size(0)    accs.append(acc)    f1_micro, f1_macro = calc_f1(data.test_y.cpu().detach().numpy(), pred.cpu().detach().numpy())    return accs, f1_micro, f1_macro